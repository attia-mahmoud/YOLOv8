{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc3142c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44052911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained YOLO model (recommended for training)\n",
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6583efa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\attim\\code\\YOLO\\people1.jpeg: 448x640 5 persons, 1 tie, 6 cups, 1 dining table, 1 laptop, 1 keyboard, 1 cell phone, 1 book, 190.2ms\n",
      "Speed: 299.6ms preprocess, 190.2ms inference, 8.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Perform object detection on an image using the model\n",
    "results = model(['people1.jpeg', ], save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa0f310f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1: https://www.youtube.com/watch?v=uWQ_8CtvzoY... Success  (461 frames of shape 1920x1080 at 25.00 FPS)\n",
      "\n",
      "\n",
      "WARNING  inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "0: 384x640 6 persons, 1 laptop, 8 books, 92.9ms\n",
      "0: 384x640 6 persons, 1 laptop, 8 books, 91.3ms\n",
      "0: 384x640 5 persons, 1 dining table, 1 laptop, 6 books, 79.0ms\n",
      "0: 384x640 4 persons, 1 dining table, 1 laptop, 6 books, 80.0ms\n",
      "0: 384x640 4 persons, 1 laptop, 6 books, 79.6ms\n",
      "0: 384x640 6 persons, 1 laptop, 6 books, 78.7ms\n",
      "0: 384x640 5 persons, 1 laptop, 5 books, 79.5ms\n",
      "0: 384x640 4 persons, 1 laptop, 6 books, 78.4ms\n",
      "0: 384x640 4 persons, 1 laptop, 5 books, 81.6ms\n",
      "0: 384x640 4 persons, 1 laptop, 5 books, 34.5ms\n",
      "0: 384x640 4 persons, 1 dining table, 1 laptop, 6 books, 34.1ms\n",
      "0: 384x640 4 persons, 1 laptop, 7 books, 34.4ms\n",
      "0: 384x640 4 persons, 1 dining table, 1 laptop, 5 books, 31.6ms\n",
      "0: 384x640 5 persons, 1 laptop, 5 books, 34.3ms\n",
      "0: 384x640 4 persons, 1 laptop, 5 books, 36.0ms\n",
      "0: 384x640 5 persons, 1 dining table, 1 laptop, 5 books, 35.4ms\n",
      "0: 384x640 4 persons, 1 laptop, 5 books, 28.6ms\n",
      "0: 384x640 5 persons, 1 dining table, 1 laptop, 5 books, 34.1ms\n",
      "0: 384x640 4 persons, 1 laptop, 5 books, 40.0ms\n",
      "0: 384x640 4 persons, 1 laptop, 5 books, 31.1ms\n",
      "0: 384x640 4 persons, 1 laptop, 5 books, 81.1ms\n",
      "0: 384x640 4 persons, 1 laptop, 5 books, 31.3ms\n",
      "0: 384x640 4 persons, 1 laptop, 5 books, 37.7ms\n",
      "0: 384x640 4 persons, 1 laptop, 5 books, 34.5ms\n",
      "0: 384x640 5 persons, 1 laptop, 5 books, 40.9ms\n",
      "0: 384x640 6 persons, 1 laptop, 6 books, 32.4ms\n",
      "0: 384x640 4 persons, 1 laptop, 7 books, 31.5ms\n",
      "0: 384x640 5 persons, 1 laptop, 7 books, 43.0ms\n",
      "0: 384x640 5 persons, 1 laptop, 6 books, 30.7ms\n",
      "0: 384x640 5 persons, 1 dining table, 1 laptop, 6 books, 34.6ms\n",
      "0: 384x640 6 persons, 1 dining table, 1 laptop, 7 books, 31.9ms\n",
      "0: 384x640 5 persons, 1 dining table, 1 laptop, 6 books, 36.0ms\n",
      "0: 384x640 7 persons, 1 dining table, 1 laptop, 6 books, 89.4ms\n",
      "0: 384x640 8 persons, 1 laptop, 7 books, 39.9ms\n",
      "0: 384x640 6 persons, 1 laptop, 6 books, 31.6ms\n",
      "0: 384x640 8 persons, 1 laptop, 7 books, 36.4ms\n",
      "0: 384x640 6 persons, 1 laptop, 8 books, 37.7ms\n",
      "0: 384x640 6 persons, 1 laptop, 7 books, 32.4ms\n",
      "0: 384x640 7 persons, 1 laptop, 7 books, 34.1ms\n",
      "0: 384x640 7 persons, 1 laptop, 7 books, 35.7ms\n",
      "Speed: 16.3ms preprocess, 47.9ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define source as YouTube video URL\n",
    "source = 'https://www.youtube.com/watch?v=uWQ_8CtvzoY'\n",
    "\n",
    "# Run inference on the source\n",
    "results = model(source, stream=True, save=True)\n",
    "\n",
    "for r in results:\n",
    "        boxes = r.boxes  # Boxes object for bbox outputs\n",
    "        masks = r.masks  # Masks object for segment masks outputs\n",
    "        probs = r.probs  # Class probabilities for classification outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd9c12d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a6cb43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1: 0... Success  (inf frames of shape 640x480 at 30.00 FPS)\n",
      "\n",
      "\n",
      "WARNING  inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "0: 480x640 1 person, 14.9ms\n",
      "0: 480x640 1 person, 14.1ms\n",
      "0: 480x640 1 person, 10.0ms\n",
      "WARNING  Waiting for stream 0\n",
      "0: 480x640 1 person, 11.7ms\n",
      "0: 480x640 1 person, 19.6ms\n",
      "0: 480x640 1 person, 12.0ms\n",
      "0: 480x640 1 person, 8.8ms\n",
      "0: 480x640 1 person, 16.9ms\n",
      "0: 480x640 1 person, 5.0ms\n",
      "WARNING  Waiting for stream 0\n",
      "0: 480x640 1 person, 8.1ms\n",
      "0: 480x640 1 person, 3.6ms\n",
      "0: 480x640 1 person, 13.1ms\n",
      "0: 480x640 1 person, 11.5ms\n",
      "0: 480x640 1 person, 9.4ms\n",
      "0: 480x640 1 person, 10.2ms\n",
      "0: 480x640 1 person, 13.8ms\n",
      "0: 480x640 1 person, 10.0ms\n",
      "0: 480x640 1 person, 9.0ms\n",
      "0: 480x640 1 person, 12.9ms\n",
      "0: 480x640 1 person, 10.0ms\n",
      "0: 480x640 1 person, 5.2ms\n",
      "0: 480x640 1 person, 13.3ms\n",
      "0: 480x640 1 person, 15.4ms\n",
      "0: 480x640 1 person, 10.0ms\n",
      "0: 480x640 1 person, 13.1ms\n",
      "0: 480x640 1 person, 16.0ms\n",
      "0: 480x640 1 person, 12.3ms\n",
      "0: 480x640 1 person, 0.0ms\n",
      "0: 480x640 1 person, 15.2ms\n",
      "0: 480x640 1 person, 14.3ms\n",
      "0: 480x640 1 person, 4.9ms\n",
      "0: 480x640 1 person, 17.4ms\n",
      "0: 480x640 1 person, 1.4ms\n",
      "0: 480x640 1 person, 4.3ms\n",
      "0: 480x640 1 person, 10.0ms\n",
      "0: 480x640 1 person, 17.4ms\n",
      "0: 480x640 1 person, 15.1ms\n",
      "0: 480x640 1 person, 5.0ms\n",
      "WARNING  Waiting for stream 0\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 11.0ms\n",
      "0: 480x640 1 person, 13.9ms\n",
      "0: 480x640 1 person, 18.6ms\n",
      "0: 480x640 1 person, 6.5ms\n",
      "0: 480x640 1 person, 15.6ms\n",
      "0: 480x640 1 person, 4.1ms\n",
      "0: 480x640 1 person, 13.3ms\n",
      "WARNING  Waiting for stream 0\n",
      "0: 480x640 1 person, 10.0ms\n",
      "0: 480x640 1 person, 16.2ms\n",
      "0: 480x640 1 person, 11.0ms\n",
      "0: 480x640 1 person, 10.0ms\n",
      "0: 480x640 1 person, 21.3ms\n",
      "0: 480x640 1 person, 16.9ms\n",
      "0: 480x640 1 person, 13.0ms\n",
      "0: 480x640 1 person, 15.4ms\n",
      "0: 480x640 1 person, 10.5ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 11.0ms\n",
      "0: 480x640 1 person, 10.0ms\n",
      "0: 480x640 1 person, 10.0ms\n",
      "0: 480x640 1 person, 12.0ms\n",
      "0: 480x640 1 person, 16.1ms\n",
      "0: 480x640 1 person, 60.0ms\n",
      "0: 480x640 1 person, 6.3ms\n",
      "0: 480x640 1 person, 10.1ms\n",
      "0: 480x640 1 person, 18.4ms\n",
      "0: 480x640 1 person, 10.8ms\n",
      "0: 480x640 1 person, 10.5ms\n",
      "0: 480x640 1 person, 16.0ms\n",
      "0: 480x640 1 person, 22.3ms\n",
      "0: 480x640 1 person, 0.0ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 14.1ms\n",
      "0: 480x640 1 person, 12.0ms\n",
      "0: 480x640 1 person, 17.3ms\n",
      "0: 480x640 1 person, 13.9ms\n",
      "0: 480x640 1 person, 13.0ms\n",
      "0: 480x640 1 person, 13.2ms\n",
      "0: 480x640 1 person, 16.9ms\n",
      "0: 480x640 1 person, 19.6ms\n",
      "0: 480x640 1 person, 14.5ms\n",
      "0: 480x640 1 person, 18.0ms\n",
      "0: 480x640 1 person, 15.0ms\n",
      "0: 480x640 1 person, 14.7ms\n",
      "0: 480x640 1 person, 19.6ms\n",
      "0: 480x640 1 person, 0.0ms\n",
      "0: 480x640 1 person, 15.2ms\n",
      "0: 480x640 1 person, 16.8ms\n",
      "0: 480x640 1 person, 15.5ms\n",
      "0: 480x640 1 person, 15.0ms\n",
      "0: 480x640 1 person, 17.1ms\n",
      "0: 480x640 1 person, 23.0ms\n",
      "0: 480x640 1 person, 17.2ms\n",
      "0: 480x640 1 person, 15.1ms\n",
      "0: 480x640 1 person, 14.1ms\n",
      "0: 480x640 1 person, 14.4ms\n",
      "0: 480x640 1 person, 15.9ms\n",
      "0: 480x640 1 person, 24.4ms\n",
      "0: 480x640 1 person, 12.8ms\n",
      "0: 480x640 1 person, 18.9ms\n",
      "0: 480x640 1 person, 14.0ms\n",
      "0: 480x640 1 person, 19.5ms\n",
      "0: 480x640 1 person, 14.1ms\n",
      "0: 480x640 1 person, 16.2ms\n",
      "0: 480x640 1 person, 13.6ms\n",
      "0: 480x640 1 person, 13.0ms\n",
      "0: 480x640 1 person, 15.0ms\n",
      "0: 480x640 1 person, 25.8ms\n",
      "0: 480x640 1 person, 12.6ms\n",
      "0: 480x640 1 person, 16.0ms\n",
      "0: 480x640 1 person, 14.2ms\n",
      "0: 480x640 1 person, 14.8ms\n",
      "0: 480x640 1 person, 12.0ms\n",
      "0: 480x640 1 person, 14.0ms\n",
      "0: 480x640 1 person, 17.7ms\n",
      "0: 480x640 1 person, 11.5ms\n",
      "0: 480x640 1 person, 14.1ms\n",
      "0: 480x640 1 person, 14.0ms\n",
      "0: 480x640 1 person, 14.8ms\n",
      "0: 480x640 1 person, 13.5ms\n",
      "0: 480x640 1 person, 17.0ms\n",
      "0: 480x640 1 person, 0.0ms\n",
      "0: 480x640 1 person, 14.9ms\n",
      "0: 480x640 1 person, 14.2ms\n",
      "0: 480x640 1 person, 12.1ms\n",
      "0: 480x640 1 person, 12.0ms\n",
      "0: 480x640 1 person, 19.3ms\n",
      "0: 480x640 1 person, 1.0ms\n",
      "0: 480x640 1 person, 17.0ms\n",
      "0: 480x640 1 person, 15.5ms\n",
      "0: 480x640 1 person, 13.9ms\n",
      "0: 480x640 1 person, 14.0ms\n",
      "0: 480x640 1 person, 15.6ms\n",
      "0: 480x640 1 person, 13.4ms\n",
      "0: 480x640 1 person, 13.1ms\n",
      "0: 480x640 1 person, 14.1ms\n",
      "0: 480x640 1 person, 18.3ms\n",
      "0: 480x640 1 person, 16.4ms\n",
      "0: 480x640 1 person, 16.8ms\n",
      "0: 480x640 1 person, 13.8ms\n",
      "0: 480x640 1 person, 19.4ms\n",
      "0: 480x640 1 person, 12.7ms\n",
      "0: 480x640 1 person, 13.0ms\n",
      "0: 480x640 1 person, 15.8ms\n",
      "0: 480x640 1 person, 15.2ms\n",
      "0: 480x640 1 person, 12.4ms\n",
      "0: 480x640 1 person, 17.9ms\n",
      "0: 480x640 1 person, 14.3ms\n",
      "0: 480x640 1 person, 13.2ms\n",
      "0: 480x640 1 person, 16.0ms\n",
      "0: 480x640 1 person, 15.2ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:248\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:194\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:56\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 56\u001b[0m                 response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:257\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n\u001b[1;32m--> 257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(preds, im, im0s)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_predict_postprocess_end\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;66;03m# Visualize, save, write results\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\models\\yolo\\detect\\predict.py:25\u001b[0m, in \u001b[0;36mDetectionPredictor.postprocess\u001b[1;34m(self, preds, img, orig_imgs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpostprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds, img, orig_imgs):\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Post-processes predictions and returns a list of Results objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     preds \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mnon_max_suppression(preds,\n\u001b[0;32m     26\u001b[0m                                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mconf,\n\u001b[0;32m     27\u001b[0m                                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39miou,\n\u001b[0;32m     28\u001b[0m                                     agnostic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39magnostic_nms,\n\u001b[0;32m     29\u001b[0m                                     max_det\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_det,\n\u001b[0;32m     30\u001b[0m                                     classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mclasses)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orig_imgs, \u001b[38;5;28mlist\u001b[39m):  \u001b[38;5;66;03m# input images are a torch.Tensor, not a list\u001b[39;00m\n\u001b[0;32m     33\u001b[0m         orig_imgs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_torch2numpy_batch(orig_imgs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\utils\\ops.py:189\u001b[0m, in \u001b[0;36mnon_max_suppression\u001b[1;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det, nc, max_time_img, max_nms, max_wh)\u001b[0m\n\u001b[0;32m    187\u001b[0m nm \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m nc \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m    188\u001b[0m mi \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m+\u001b[39m nc  \u001b[38;5;66;03m# mask start index\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m xc \u001b[38;5;241m=\u001b[39m prediction[:, \u001b[38;5;241m4\u001b[39m:mi]\u001b[38;5;241m.\u001b[39mamax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m conf_thres  \u001b[38;5;66;03m# candidates\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Settings\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# min_wh = 2  # (pixels) minimum box width and height\u001b[39;00m\n\u001b[0;32m    193\u001b[0m time_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m+\u001b[39m max_time_img \u001b[38;5;241m*\u001b[39m bs  \u001b[38;5;66;03m# seconds to quit after\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    results = model.predict(source=\"0\", show=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f47ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
